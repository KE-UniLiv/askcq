# AskCQ
A Multi-Annotator Comparative Evaluation of Competency Question Formulation Approaches

This repository contains the code and experimental data for the research paper evaluating different methodologies for formulating Competency Questions (CQs) in ontology engineering. The project, `AskCQ`, investigates how the choice of formulation method impacts the quality and utility of CQs derived from user stories. We compare three approaches: manual formulation by ontology engineers, instantiation of predefined CQ templates, and automated generation using Large Language Models (LLMs). Using user stories from the British Music Experience (BME) cultural heritage domain as a case study, we generated sets of CQs via each method. The resulting CQs were evaluated through a multi-annotator process assessing their perceived utility for ontology engineering tasks (design, validation, testing) and quantitatively measured for clarity (via ambiguity assessment), relevance to the source user story, readability (using standard indices), and complexity (approximated by length and algorithmic steps). The code includes scripts for processing user stories, generating CQs (e.g., LLM prompts/interactions, template logic), running evaluation metrics, performing analysis, and potentially replicating the results presented in our accompanying publication.